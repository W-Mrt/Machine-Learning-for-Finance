{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMROWV3+RKe+/K8mdLlyNrB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/W-Mrt/Machine-Learning-for-Finance/blob/main/FIN_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2yy6Yx4WExun"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow-gpu\n",
        "#!pip install tensorflow\n",
        "# !pip install keras\n",
        "# !pip install matplotlib\n",
        "# !pip install pandas\n",
        "# !pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "import tensorflow as tf\n",
        "#from tensorflow import set_random_seed\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "#from fin_data import data_generator\n",
        "from keras.models import Sequential\n",
        "# from keras.layers.merge import Add\n",
        "from keras.layers import Add\n",
        "from keras.optimizers import Adam,Nadam\n",
        "#from models import discriminator_model, generator_model_cnn,generator_model_mlp,generator_model_mlp_cnn,generator_model_mlp_cnn_plus\n",
        "import matplotlib.pyplot as plt\n",
        "#from stats import acf\n",
        "import numpy as np\n",
        "from numpy.random import seed\n",
        "#import visualize\n",
        "#import stylized_facts as sf\n",
        "from datetime import datetime as dt\n",
        "import os\n",
        "from shutil import copyfile\n",
        "import argparse "
      ],
      "metadata": {
        "id": "t2uLsJsOGd-Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def auto_correlation(x,lag = 1):\n",
        "\ta = pd.Series(np.reshape(x,(-1)))\n",
        "\tb = a.autocorr(lag = lag)\n",
        "\tif np.isnan(b) or np.isinf(b):\n",
        "\t\treturn 0\n",
        "\treturn b\n",
        "\n",
        "def acf(x,max_lag=1000):\n",
        "\tacf = []\n",
        "\tfor i in range(max_lag):\n",
        "\t\tacf.append(auto_correlation(x,lag=i+1))\n",
        "\treturn np.array(acf)"
      ],
      "metadata": {
        "id": "0G6aU9buOC-m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizer\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib as mpl\n",
        "#import stats\n",
        "import numpy as np\n",
        "\n",
        "def matplotlib_config():\n",
        "\t\tmpl.rc('text', color='#3ba6d8')\n",
        "\t\tmpl.rc('axes', facecolor='none',edgecolor=\"#3ba6d8\",labelcolor=\"#3ba6d8\",titlesize=15,labelsize=10,grid=True)\n",
        "\t\tmpl.rc('lines', linewidth=1,color=\"#3ba6d8\")\n",
        "\t\tmpl.rc('grid',color='#f2f2f2',alpha=0.3,linewidth=1)\n",
        "\t\tmpl.rc('xtick',color='#3ba6d8')\n",
        "\t\tmpl.rc('ytick',color='#3ba6d8')\n",
        "\n",
        "def basic_graph(x,title=''):\n",
        "\tmatplotlib_config()\n",
        "\tplt.figure()\n",
        "\tplt.plot(x,color='red')\n",
        "\tplt.legend(handles=stat_patch_list(x),fontsize=10,loc='upper right')\n",
        "\t#plt.ylim(-0.5,1.0)\n",
        "\tplt.savefig(title +'.png',transparent=True)\n",
        "\tplt.close()\n",
        "\n",
        "def time_series_graph(x,title=''):\n",
        "\tmatplotlib_config()\n",
        "\tplt.figure()\n",
        "\tplt.plot(x,color='red')\n",
        "\tplt.savefig(title +'.png',transparent=True)\n",
        "\tplt.close()\n",
        "\n",
        "def graph_with_subsampling(x,subsample_list,title=''):\n",
        "\tmatplotlib_config()\n",
        "\tplt.figure()\n",
        "\tplt.plot(x,color='red')\n",
        "\tfor sample in subsample_list:\n",
        "\t\tplt.plot(sample,color='blue')\n",
        "\tplt.legend(handles=stat_patch_list(x),fontsize=10,loc='upper right')\n",
        "\tplt.savefig(title +'.png',transparent=True)\n",
        "\tplt.close()\n",
        "\n",
        "def prediction_graph(pred,y,title=''):\n",
        "\tmatplotlib_config()\n",
        "\tplt.figure()\n",
        "\tplt.plot(y,color='blue')\n",
        "\tplt.plot(pred,color='red')\n",
        "\tplt.title(title)\n",
        "\t#plt.xlabel('data in series')\n",
        "\t#plt.ylabel('std21')\n",
        "\tplt.legend(handles=comparing_stat_patch_list(pred,y),fontsize=10,loc='upper right')\n",
        "\tplt.savefig(title +'.png',transparent=True)\n",
        "\tplt.close()\n",
        "\n",
        "def histogram_graph(y,title=''):\n",
        "\tmatplotlib_config()\n",
        "\tplt.figure()\n",
        "\tplt.hist(y,bins=100,color='blue')\n",
        "\tplt.title(title)\n",
        "\tplt.legend(handles=stat_patch_list(y),fontsize=10,loc='upper right')\n",
        "\tplt.savefig(title +'.png',transparent=True)\n",
        "\tplt.close()\n",
        "\n",
        "def comparing_stat_patch_list(pred,y):\n",
        "\tpatch1 = mpatches.Patch(color='red', label= 'mean:'+ ('%03.6f' % np.mean(pred)))\n",
        "\tpatch2 = mpatches.Patch(color='red', label= 'std:'+ ('%03.6f' % np.std(pred)))\n",
        "\tpatch3 = mpatches.Patch(color='red', label= 'skewness:'+ ('%03.3f' % stats.skewness(pred)))\n",
        "\tpatch4 = mpatches.Patch(color='red', label= 'kurtosis:'+ ('%03.3f' % stats.kurtosis(pred)))\n",
        "\tpatch5 = mpatches.Patch(color='blue', label= 'mean:'+ ('%03.6f' % np.mean(y)))\n",
        "\tpatch6 = mpatches.Patch(color='blue', label= 'std:'+ ('%03.6f' % np.std(y)))\n",
        "\tpatch7 = mpatches.Patch(color='blue', label= 'skewness:'+ ('%03.3f' % stats.skewness(y)))\n",
        "\tpatch8 = mpatches.Patch(color='blue', label= 'kurtosis:'+ ('%03.3f' % stats.kurtosis(y)))\n",
        "\t#patch9 = mpatches.Patch(color='black', label= 'MAPE:'+ ('%03.3f' % stats.mape(pred, y)))\n",
        "\tpatch10 = mpatches.Patch(color='black', label= 'RMSE:'+ ('%03.6f' % stats.rmse(pred, y)))\n",
        "\t#plt.text(.25,.5,str(np.mean(pred)))\n",
        "\treturn [patch5,patch6,patch7,patch8,patch1,patch2,patch3,patch4,patch10]\n",
        "\n",
        "def stat_patch_list(x):\n",
        "\tpatch0 = mpatches.Patch(label= 'data num:'+ str((x.size)))\n",
        "\tpatch1 = mpatches.Patch(label= 'mean:'+ ('%03.6f' % np.mean(x)))\n",
        "\tpatch2 = mpatches.Patch(label= 'std:'+ ('%03.6f' % np.std(x)))\n",
        "\tpatch3 = mpatches.Patch(label= 'skewness:'+ ('%03.3f' % stats.skewness(x)))\n",
        "\tpatch4 = mpatches.Patch(label= 'kurtosis:'+ ('%03.3f' % stats.kurtosis(x)))\n",
        "\treturn [patch0,patch1,patch2,patch3,patch4]\n",
        "\n",
        "def basic_patch_list():\n",
        "\tpatch1 = mpatches.Patch(color='red', label= 'prediciton')\n",
        "\tpatch2 = mpatches.Patch(color='blue', label= 'actual value')\n",
        "\treturn [patch1,patch2]\n",
        "\n",
        "def patch_list(pred,y):\n",
        "\treturn stat_patch_list(pred, y)"
      ],
      "metadata": {
        "id": "mwhJSNRvNkFB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import arch\n",
        "import os\n",
        "#import stats\n",
        "#import visualizer\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "#import talib as ta\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class data_manager():\n",
        "\tdef __init__(self):\n",
        "\t\tself.csv_file = ''\n",
        "\t\tpass\n",
        "\n",
        "#where to normalize?\n",
        "\tdef prepare_pd(self):\n",
        "\t\tself.read_csv()\n",
        "\t\tself.parse_date()\n",
        "\t\tself.data_begin_date = None\n",
        "\t\tself.data_end_date = None\n",
        "\t\tself.add_columns()\n",
        "\n",
        "\tdef add_columns(self,is_financial = True, contain_adjusted = True,contain_volume=True,validation_split=0.81):\n",
        "\t\tif is_financial:\n",
        "\t\t\tif contain_adjusted:\n",
        "\t\t\t\tself.data['Log Return'] = np.log(self.data['Adj Close']) - np.log(self.data['Adj Close'].shift(1))\n",
        "\t\t\t\tself.data['Pct Change'] = self.data['Adj Close'].pct_change().dropna()\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.data['Log Return'] = np.log(self.data['Close']) - np.log(self.data['Close'].shift(1))\n",
        "\t\t\t\tself.data['Pct Change'] = self.data['Close'].pct_change().dropna()\n",
        "\t\t\t# self.data['U'] = np.log(self.data['High']/self.data['Open'])\n",
        "\t\t\t# self.data['D'] = np.log(self.data['Low']/self.data['Open'])\n",
        "\t\t\t# self.data['C'] = np.log(self.data['Close']/self.data['Open'])\n",
        "\t\t\t# self.data['A'] = self.data.apply(lambda x: 1 if x['Log Return'] >=0 else 0 ,axis=1)\n",
        "\t\t\t# self.data['Y'] = self.data['A'].shift(-1)\n",
        "\t\t\t# self.data['Daily Volatility'] = np.sqrt(0.511*((self.data['U']-self.data['D'])**2) - 0.019*(self.data['C']*(self.data['U']+self.data['D'])-2*self.data['U']*self.data['D']) - 0.383*(self.data['C']**2))\n",
        "\t\t\t# self.data['Simple 10-day MA'] = pd.rolling_mean(self.data['Close'],10)\n",
        "\t\t\t# self.data['Weighted 10-day MA'] = ta.WMA(np.array(self.data['Close'],dtype=np.float),timeperiod=10)\n",
        "\t\t\t# self.data['Momentum'] = ta.MOM(np.array(self.data['Close'],dtype=np.float), timeperiod=10)\n",
        "\t\t\t# self.data['Stochastic K'] = stats.stochastic_K(self.data['Close'], self.data['High'], self.data['Low'], 14)\n",
        "\t\t\t# self.data['Stochastic D'] = stats.stochastic_D(self.data['Close'], self.data['High'], self.data['Low'], 14,3)\n",
        "\t\t\t# self.data['sign'] = self.data.apply(lambda x: 1 if x['Log Return'] >=0 else -1 ,axis=1)\n",
        "\t\t\t# self.data['RSI'] = ta.RSI(np.array(self.data['Close'],dtype=np.float), timeperiod=14)\n",
        "\t\t\t# self.data['MACD'] = ta.MACD(np.array(self.data['Close'],dtype=np.float),fastperiod=12, slowperiod=26, signalperiod=9)[0]\n",
        "\t\t\t# self.data['Larry William R'] = ta.WILLR(np.array(self.data['High'],dtype=np.float), np.array(self.data['Low'],dtype=np.float),np.array(self.data['Close'],dtype=np.float), timeperiod=14)\n",
        "\t\t\t# self.data['AD'] = stats.AD(self.data['Close'],self.data['High'],self.data['Low'])\n",
        "\t\t\t# self.data['CCI'] = ta.CCI(np.array(self.data['Close'],dtype=np.float), np.array(self.data['Low'],dtype=np.float), np.array(self.data['High'],dtype=np.float), timeperiod=20)\n",
        "\t\t\t# if contain_volume:\n",
        "\t\t\t# \tself.data['Mean Volume 300'] = pd.rolling_mean(self.data['Volume'], 300)\n",
        "\t\t\t# \tself.data['Std Volume 300'] = pd.rolling_std(self.data['Volume'], 300)\n",
        "\t\t\t# \tself.data['Normalized Volume 300'] = (self.data['Volume']-self.data['Mean Volume 300'])/self.data['Std Volume 300']\n",
        "\t\t\t\t#self.data['Mean Volume 1000'] = pd.rolling_mean(self.data['Volume'], 1000)\n",
        "\t\t\t\t#self.data['Std Volume 1000'] = pd.rolling_std(self.data['Volume'], 1000)\n",
        "\t\t\t\t#self.data['Normalized Volume 1000'] = (self.data['Volume']-self.data['Mean Volume 1000'])/self.data['Std Volume 1000']\n",
        "\t\t\t# self.data['Stdev2'] = pd.rolling_std(self.data['Log Return'], 2)\n",
        "\t\t\t# self.data['Stdev7'] = pd.rolling_std(self.data['Log Return'], 7)\n",
        "\t\t\t# self.data['Stdev14'] = pd.rolling_std(self.data['Log Return'], 14)\n",
        "\t\t\t# self.data['Stdev21'] = pd.rolling_std(self.data['Log Return'], 21)\n",
        "\t\t\t# self.data['Stdev28'] = pd.rolling_std(self.data['Log Return'], 28)\n",
        "\t\t\t#GARCH fitting\n",
        "\t\t\tself.data = self.data.dropna()\n",
        "\t\t\t#\n",
        "\t\t\t# self.data['Forecast Volatility'] = np.sqrt(res.params['omega'] + res.params['alpha[1]'] * res.resid**2 + res.conditional_volatility**2 * res.params['beta[1]'])\n",
        "\t\t\t# self.data['Annualized Forecast Volatility'] = self.data['Forecast Volatility']*np.sqrt(252)\n",
        "\n",
        "\tdef fill_garch(self,ret,validation_split=0.2):\n",
        "\t\tc = 100\n",
        "\t\tam = arch.arch_model(ret[:int((1-validation_split)*ret.size)]*c)\n",
        "\t\tres = am.fit(update_freq=5,disp='on')\n",
        "\t\tnew_cond_vol = np.zeros(ret.size)\n",
        "\t\tc_vol = res.conditional_volatility\n",
        "\t\tfor i in range(ret.size):\n",
        "\t\t\tif c_vol.size > i:\n",
        "\t\t\t\tnew_cond_vol[i] = c_vol[i]\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_cond_vol[i] = np.sqrt(res.params['omega'] + res.params['alpha[1]'] * ((ret[i-1]*c)-res.params['mu'])**2 + (new_cond_vol[i - 1]**2)*res.params['beta[1]'])\n",
        "\t\treturn new_cond_vol/c\n",
        "\t\t\t#return None\n",
        "\tdef read_csv(self):\n",
        "\t\tself.data = pd.read_csv(self.csv_file)\n",
        "\n",
        "\tdef data_stat(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef get_pd_table(self,**kwargs):\n",
        "\t\ttable = self.data\n",
        "\t\tif 'date_start' in kwargs:\n",
        "\t\t\ttable = table.ix[table['Date'] > kwargs['date_start']]\n",
        "\t\t\tkwargs.pop('date_start')\n",
        "\t\tif 'date_end' in kwargs:\n",
        "\t\t\ttable = table.ix[self.data['Date'] < kwargs['date_end']]\n",
        "\t\t\tkwargs.pop('date_end')\n",
        "\n",
        "\t\treturn table\n",
        "\n",
        "\tdef get_dataset(self,column_list,look_back = 10,normalize_width = 0,normalize_scheme = None,train_ratio = 0.8):\n",
        "\t\t#get specified pd table\n",
        "\t\tspecified_table = self.data(column_list)\n",
        "\t\t#Normalize\n",
        "\t\tif normalize_width:\n",
        "\t\t\tfor e in normalize_scheme:\n",
        "\t\t\t\tpass\n",
        "\t\t#Look-Back\n",
        "\n",
        "\t\t#Divide data into train/test\n",
        "\t\ttrain_x = None\n",
        "\t\ttrain_y = None\n",
        "\t\ttest_x = None\n",
        "\t\ttest_y = None\n",
        "\t\treturn self._create_dataset(train_x,train_y),self._create_dataset(test_x,test_y)\n",
        "\t\tpass\n",
        "\n",
        "\tdef create_dataset(self,x_set, y_set,look_back=10):\n",
        "\t\tdata_x, data_y = [], []\n",
        "\t\tfor i in range(len(x_set)-look_back-1):\n",
        "\t\t\ta = x_set[i:(i+look_back)]\n",
        "\t\t\tdata_x.append(a)\n",
        "\t\t\tdata_y.append(y_set[i + look_back - 1])\n",
        "\t\treturn np.array(data_x), np.array(data_y)\n",
        "\n",
        "\tdef parse_date(self):\n",
        "\t\tself.data['Date'] = pd.to_datetime(self.data['Date'])\n",
        "\t\toldtime_table = self.data.copy()\n",
        "\t\toldtime_table['Date'] -= pd.Timedelta(100,'Y')\n",
        "\t\tself.data = self.data.where(self.data['Date'] < pd.to_datetime('2017/1/1'),oldtime_table)\n",
        "\n",
        "class gdc(data_manager):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.csv_file = ''\n",
        "\n",
        "\tdef prepare_pd(self):\n",
        "\t\tself.data = pd.read_csv(self.csv_file)\n",
        "\n",
        "\n",
        "class snp500(data_manager):\n",
        "\tdef __init__(self):\n",
        "\t\tself.csv_file = './data/SNP500/snp500.csv'\n",
        "\n",
        "\tdef get_code_list(self):\n",
        "\t\tt = pd.read_csv('./data/SNP500/SNP500_Individuals/constituents.csv')\n",
        "\t\treturn t['Symbol'].as_matrix()\n",
        "\n",
        "\n",
        "class snp500_individual(data_manager):\n",
        "\tdef __init__(self,code):\n",
        "\t\t#os.chdir(\"./data/SNP500/SNP500_Individuals\")\n",
        "\t\tself.csv_file = \"./data/SNP500/SNP500_Individuals/\" +  str(code) + '.csv'\n",
        "\n",
        "\tdef read_csv(self):\n",
        "\t\tself.data = pd.read_csv(self.csv_file,dtype={'Open': np.float32,'Low': np.float32,'High': np.float32,'Close': np.float32,'Volume': np.float32,'Adj_Close': np.float32})\n",
        "\t\tself.data.rename(columns={'Adj_Close':'Adj Close'},inplace=True)\n",
        "\t\t#self.data = self.data.iloc[::-1]\n",
        "\n",
        "\n",
        "\n",
        "class nikkei225_individual(data_manager):\n",
        "\tdef __init__(self,code):\n",
        "\t\t#os.chdir(\"./data/Nikkei225/Nikkei225_Individuals\")\n",
        "\t\tself.csv_file = \"../data/Nikkei225/Nikkei225_Individuals/\" + str(code) + '.csv'\n",
        "\n",
        "\tdef read_csv(self):\n",
        "\t\tself.data = pd.read_csv(self.csv_file,names=['Date','Open','High','Low','Close','Volume','Adj Close'],dtype={'Open': np.float32,'Low': np.float32,'High': np.float32,'Close': np.float32,'Volume': np.float32,'Adj Close': np.float32})\n",
        "\t\tself.data = self.data.iloc[::-1]\n",
        "\n",
        "class nikkei225(data_manager):\n",
        "\tdef __init__(self):\n",
        "\t\t#os.chdir(\"./data/Nikkei225\")\n",
        "\t\tself.csv_file = '../data/Nikkei225/nikkei 225.csv'\n",
        "\n",
        "\tdef prepare_pd(self):\n",
        "\t\tself.read_csv()\n",
        "\t\tself.data_begin_date = None\n",
        "\t\tself.data_end_date = None\n",
        "\t\tself.add_columns(True,False,False)\n",
        "\n",
        "\tdef read_csv(self):\n",
        "\t\t#self.data = pd.read_csv(self.csv_file,names=['Date','Open','Low','High','Close'],dtype={'Open': np.float32,'Low': np.float32,'High': np.float32,'Close': np.float32})\n",
        "\t\tself.data = pd.read_csv(self.csv_file)\n",
        "\t\tself.data = self.data.rename(index=str, columns={\"date\": \"Date\", \"open\": \"Open\",\"low\": \"Low\",\"high\": \"High\",\"close\": \"Close\"})\n",
        "\n",
        "\tdef get_code_list(self):\n",
        "\t\tt = pd.read_csv(\"../data/Nikkei225/Nikkei225_Individuals/nikkei225-stock-prices.csv\")\n",
        "\t\treturn t['SC'].as_matrix()\n",
        "\n",
        "class djia(data_manager):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.csv_file = ''"
      ],
      "metadata": {
        "id": "XrVlBbUbNaTW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fin_data\n",
        "import sys\n",
        "#sys.path.append('./lib')\n",
        "import numpy as np\n",
        "#from data import snp500,snp500_individual\n",
        "\n",
        "# s.prepare_pd()\n",
        "# table = s.get_pd_table()\n",
        "# size = table['Log Return'].as_matrix().size\n",
        "# k = table['Log Return'].as_matrix()\n",
        "\n",
        "class data_generator():\n",
        "        def __init__(self):\n",
        "                self.sequence_length = 1024*8\n",
        "                self.batch_size = 12\n",
        "                self.learning_phase = \"train\"\n",
        "                self.train_split = 0.8\n",
        "\n",
        "        def snp500_index(self):\n",
        "                s = snp500()\n",
        "                s.prepare_pd()\n",
        "                table = s.get_pd_table()\n",
        "                return table['Log Return'].as_matrix()\n",
        "        def real_data(self,mode = \"individuals\"):\n",
        "                if mode == \"individuals\":\n",
        "                        return self.individuals_data_random_picker()\n",
        "                elif mode == \"index\":\n",
        "                        return self.index_data_random_picker()\n",
        "\n",
        "        def individuals_data_random_picker(self):\n",
        "                data = []\n",
        "                for i in range(self.batch_size):\n",
        "                        random_code = self.choose_random_code()\n",
        "                        si = snp500_individual(random_code)\n",
        "                        si.prepare_pd()\n",
        "                        table = si.get_pd_table()\n",
        "                        sequence = table['Log Return'].as_matrix()\n",
        "                        size = table['Log Return'].as_matrix().size\n",
        "                        random_pos = np.random.randint(0,size-self.sequence_length)\n",
        "                        data.append([sequence[random_pos:random_pos+self.sequence_length]])\n",
        "                data = np.array(data)\n",
        "                #data /= max(data.max(),-data.min())\n",
        "                data = np.reshape(data,(self.batch_size,self.sequence_length,1))\n",
        "                return data\n",
        "\n",
        "        def choose_random_code(self):\n",
        "                s = snp500()\n",
        "                codes = s.get_code_list()\n",
        "                codes_size = len(codes)\n",
        "                size = 0\n",
        "                while size < self.sequence_length:\n",
        "                        if self.train_split == -1:\n",
        "                                random_code = codes[np.random.randint(0,codes_size)]\n",
        "                        else:\n",
        "                                if self.learning_phase == \"train\":\n",
        "                                        random_code = codes[np.random.randint(0,int(self.train_split*codes_size))]\n",
        "                                else:\n",
        "                                        random_code = codes[np.random.randint(int(self.train_split*codes_size),codes_size)]\n",
        "                        si = snp500_individual(random_code)\n",
        "                        si.prepare_pd()\n",
        "                        table = si.get_pd_table()\n",
        "                        size = table['Log Return'].as_matrix().size\n",
        "                return random_code\n",
        "\n",
        "        def index_data_random_picker(self):\n",
        "                s = snp500()\n",
        "                s.prepare_pd()\n",
        "                table = s.get_pd_table()\n",
        "                sequence = table['Log Return'].as_matrix()\n",
        "                size = table['Log Return'].as_matrix().size\n",
        "                data = []\n",
        "                for i in range(self.batch_size):\n",
        "                        random_pos = np.random.randint(0,size-self.sequence_length)\n",
        "                        data.append([sequence[random_pos:random_pos+self.sequence_length]])\n",
        "                data = np.array(data)\n",
        "                #data /= max(data.max(),-data.min())\n",
        "                data = np.reshape(data,(self.batch_size,self.sequence_length,1))\n",
        "                return "
      ],
      "metadata": {
        "id": "uNZkWTdDMqaf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='FIN-GAN implementation')\n",
        "parser.add_argument('--batch_size',type=int,default=24)\n",
        "parser.add_argument('--generator_model',type=str,default='mlp-cnn')\n",
        "parser.add_argument('--epochs',type=int,default=10)\n",
        "parser.add_argument('--batches',type=int,default=1024)\n",
        "parser.add_argument('--folder_name',type=str,default='')\n",
        "parser.add_argument('--generator_lr',type=float,default='2e-4')\n",
        "parser.add_argument('--discriminator_lr',type=float,default='1e-5')\n",
        "parser.add_argument('--log_interval',type=int,default=50)\n",
        "parser.add_argument('--seed',type=int,default=1)\n",
        "parser.add_argument('-f')\n",
        "\n",
        "args = parser.parse_args()\n",
        "seed(args.seed)\n",
        "set_random_seed(args.seed)"
      ],
      "metadata": {
        "id": "KbRDkAEOJanN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dg = data_generator()\n",
        "batch_size = 24\n",
        "dg.batch_size = batch_size\n",
        "batches = 1024\n",
        "epochs = 10                 \n",
        "timestamp = dt.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "timestamp += '_'\n",
        "timestamp += args.folder_name\n",
        "#os.makedirs('./imgs/%s'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./imgs/%s'%(timestamp))\n",
        "#os.makedirs('./npy/%s'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./npy/%s'%(timestamp))\n",
        "#os.makedirs('./weights/%s'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./weights/%s'%(timestamp))\n",
        "#os.makedirs('./imgs/%s/acf'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./imgs/%s/acf'%(timestamp))\n",
        "#os.makedirs('./imgs/%s/dist'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./imgs/%s/dist'%(timestamp))\n",
        "#os.makedirs('./imgs/%s/time_series'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./imgs/%s/time_series'%(timestamp))\n",
        "#os.makedirs('./imgs/%s/leverage'%(timestamp), exist_ok=True)\n",
        "os.mkdir('./imgs/%s/leverage'%(timestamp))\n",
        "copyfile('./FIN-GAN.ipynb','./imgs/%s/main.py'%(timestamp))\n",
        "#copyfile('./models.py','./imgs/%s/models.py'%(timestamp))\n",
        "with open('./imgs/%s/hyper_parameters.txt'%(timestamp),'w') as w:\n",
        "    w.write(str(args))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kLcXjFV1KGoe",
        "outputId": "6a4f1f08-83f9-4658-cef2-80d77911b50b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0ec87226db2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#os.makedirs('./imgs/%s/leverage'%(timestamp), exist_ok=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./imgs/%s/leverage'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./FIN-GAN.ipynb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./imgs/%s/main.py'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#copyfile('./models.py','./imgs/%s/models.py'%(timestamp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./imgs/%s/hyper_parameters.txt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './FIN-GAN.ipynb'"
          ]
        }
      ]
    }
  ]
}